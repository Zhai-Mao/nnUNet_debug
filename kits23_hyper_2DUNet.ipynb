{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35246d1-3d3f-41b6-84c4-32b86e0b8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae84252-9111-4ece-8f05-a1d9dacf3606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa61e51-c644-4ca9-8a69-40b2d5c9ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plan and preprocesstion\n",
    "# d = 220\n",
    "# fpe = DatasetfingerPrintExtractor\n",
    "# npfp = num of preprocess used for fingerprint extraction\n",
    "# use kits23 dataset to train\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import shutil\n",
    "dataset_id = 220\n",
    "num_process = 8\n",
    "verbose = False\n",
    "# def fingerprint_extractor_class(dataset_id, num_process, verbose):\n",
    "# trans data to imageTr, labelsTr \n",
    "input_folder = 'autodl-tmp/kits19-cnn-master/kits23-main/dataset'\n",
    "out_base = 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw'\n",
    "imagesTr = join(out_base, \"imagesTr\")\n",
    "labelsTr = join(out_base, \"labelsTr\")\n",
    "\n",
    "# make dir\n",
    "maybe_mkdir_p(imagesTr)\n",
    "maybe_mkdir_p(labelsTr)\n",
    "# print(imagesTr)\n",
    "\n",
    "# require case head files\n",
    "# copy cases[0,8]\n",
    "cases = subdirs(input_folder, prefix='case_', join=False)\n",
    "\n",
    "cases_to_process = cases[:9]\n",
    "for tr in cases_to_process:\n",
    "    shutil.copy(join(input_folder, tr, 'imaging.nii.gz'), join(imagesTr, f'{tr}_0000.nii.gz'))\n",
    "    shutil.copy(join(input_folder, tr, 'segmentation.nii.gz'), join(labelsTr, f'{tr}.nii.gz'))\n",
    "# num_training_cases=len(cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0d22d2-8bdc-4981-a482-fdef80a7813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(1, 2, 3)\n",
      "(2, 3)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# generate data json\n",
    "from typing import Tuple, Union, List\n",
    "\n",
    "\n",
    "# 对labels中的字典类型进行判断\n",
    "labels = {\n",
    "    \"background\":0,\n",
    "    \"kidney\":(1,2,3),\n",
    "    \"masses\":(2,3),\n",
    "    \"tumor\": 2\n",
    "}\n",
    "# print(labels.values()) #dict_values([0, (1, 2, 3), (2, 3), 2])\n",
    "has_regions: bool = any([isinstance(i, (tuple,list)) and len(i) > 1 for i in labels.values()])\n",
    "# print(has_regions) #Ture\n",
    "\n",
    "# 标签的排列顺序\n",
    "region_class_order=(1,3,2)\n",
    "\n",
    "# 数据集的模态信息转换成channel_name = {['0':'CT'}\n",
    "channels_name={0:\"CT\"}\n",
    "keys = list(channels_name.keys())\n",
    "# print(keys)#[0]\n",
    "for k in keys:\n",
    "#     print(k)\n",
    "    if not isinstance(k, str):\n",
    "#         print(\"error\")\n",
    "        channels_name[str(k)] = channels_name[k]\n",
    "        del channels_name[k]\n",
    "# print(channels_name)#{'0': 'CT'}\n",
    "\n",
    "# 对labels中的字典类型进行判断,并转换成整数。这个要求字典类型为整数\n",
    "for l in labels.keys():\n",
    "#     print(l)\n",
    "    value = labels[l]\n",
    "#   print(values)\n",
    "    if isinstance(value, (tuple, list)):\n",
    "        value = tuple([int(i) for i in value])\n",
    "        labels[l] = value\n",
    "        print(value)\n",
    "    else:\n",
    "        labels[l] = int(labels[l])\n",
    "        print(labels[l])\n",
    "# print(labels)\n",
    "file_ending='.nii.gz'\n",
    "license: str = 'although the person is lazy, but i think the code is already very good!'\n",
    "converted_by: str= 'i dont know who write this code'\n",
    "dataset_name = \"KITS2023\"\n",
    "reference = 'none'\n",
    "release = '0.1.3'\n",
    "description = \"KITS2023\"\n",
    "overwrite_image_reader_writer = 'NibabelIOWithReoient'\n",
    "input_folder = 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw'\n",
    "cases = subdirs(input_folder, prefix='case_', join=False)\n",
    "num_training_cases=len(cases)\n",
    "dataset_json = {\n",
    "    'channel_names':channels_name,\n",
    "    'labels':labels,\n",
    "    'numTraining':num_training_cases,\n",
    "    'file_ending':file_ending,\n",
    "    'licence':license,\n",
    "    'converted_by':converted_by\n",
    "}\n",
    "if dataset_name is not None:\n",
    "    dataset_json['name'] = dataset_name\n",
    "if reference is not None:\n",
    "    dataset_json['reference']=reference\n",
    "if release is not None:\n",
    "    dataset_json['release']=release\n",
    "if description is not None:\n",
    "    dataset_json['description']=description\n",
    "if overwrite_image_reader_writer is not None:\n",
    "    dataset_json['overwrite_image_reader_writer']=overwrite_image_reader_writer\n",
    "if region_class_order is not None:\n",
    "    dataset_json['region_class_order']=region_class_order\n",
    "\n",
    "# dataset_json.update(kwargs)\n",
    "# print(dataset_json)\n",
    "out_base = 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw'\n",
    "save_json(dataset_json, join(out_base, 'dataset_json'), sort_keys=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63737be4-79d6-455b-a365-6fedbadadeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'case_00000': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00000_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00000.nii.gz'}, 'case_00001': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00001_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00001.nii.gz'}, 'case_00002': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00002_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00002.nii.gz'}, 'case_00003': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00003_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00003.nii.gz'}, 'case_00004': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00004_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00004.nii.gz'}, 'case_00005': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00005_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00005.nii.gz'}, 'case_00006': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00006_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00006.nii.gz'}, 'case_00007': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00007_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00007.nii.gz'}, 'case_00008': {'images': ['autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00008_0000.nii.gz'], 'label': 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/labelsTr/case_00008.nii.gz'}}\n"
     ]
    }
   ],
   "source": [
    "# dataset finger print\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "num_process = 8\n",
    "out_base = \"autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw\"\n",
    "dataset_json = load_json(\"autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/dataset_json\")\n",
    "# dataset = \n",
    "folder = join(out_base, \"imagesTr\")\n",
    "files = subfiles(folder, suffix=dataset_json[\"file_ending\"], join=False, sort=True)\n",
    "# print(files)\n",
    "# ['case_00000_0000.nii.gz', 'case_00001_0000.nii.gz', 'case_00002_0000.nii.gz', 'case_00003_0000.nii.gz', 'case_00004_0000.nii.gz', 'case_00005_0000.nii.gz', 'case_00006_0000.nii.gz'\n",
    "crop = len(file_ending) + 5\n",
    "files1 = [i[:-crop] for i in files]\n",
    "# ['case_00000', 'case_00001', 'case_00002', 'case_00003', 'case_00004', 'case_00005', 'case_00006', 'case_00007', 'case_00008']\n",
    "# print(files)\n",
    "\n",
    "# identifier\n",
    "files2 = np.unique(files1)\n",
    "# print(files)\n",
    "\n",
    "param_list = [(folder, files, file_ending, f) for f in files2]\n",
    "# print(param_list)\n",
    "list_of_lists = []\n",
    "\n",
    "# 匹配正则表达式筛选文件\n",
    "def create_paths_fn(folder, files, file_ending, f):\n",
    "    p = re.compile(re.escape(f) + r\"_\\d\\d\\d\\d\" + re.escape(file_ending))\n",
    "    return [join(folder, i) for i in files if p.fullmatch(i)]\n",
    "\n",
    "# images\n",
    "with Pool(processes=num_process) as pool:\n",
    "    list_of_lists = pool.starmap(create_paths_fn, param_list)\n",
    "#     print(list_of_lists)\n",
    "\n",
    "segs = [join(out_base, 'labelsTr', i + dataset_json['file_ending']) for i in files2]\n",
    "# print(segs)\n",
    "\n",
    "dataset = {i: {'images': im, 'label': se} for i, im, se in zip(files2, list_of_lists, segs)}\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "126b2348-3ab6-40c0-8f81-f6fb516f208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting acvl-utils==0.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/56/27/e64997dbcf2671639286a2d46a76fa6ef8f2c1d6f766c0dbe2a5f0b396aa/acvl_utils-0.2.tar.gz (18 kB)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.8/site-packages (from acvl-utils==0.2) (1.23.5)\n",
      "Requirement already satisfied: batchgenerators in ./miniconda3/lib/python3.8/site-packages (from acvl-utils==0.2) (0.21)\n",
      "Requirement already satisfied: torch in ./miniconda3/lib/python3.8/site-packages (from acvl-utils==0.2) (2.4.1)\n",
      "Requirement already satisfied: SimpleITK in ./miniconda3/lib/python3.8/site-packages (from acvl-utils==0.2) (2.4.1)\n",
      "Requirement already satisfied: scikit-image in ./miniconda3/lib/python3.8/site-packages (from acvl-utils==0.2) (0.21.0)\n",
      "Collecting connected-components-3d\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/41/da/0946f12c6aee8f35df9f010256ee649520397e49fabd1dbea85b004ef0c9/connected_components_3d-3.23.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: unittest2 in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (1.3.2)\n",
      "Requirement already satisfied: scipy in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (1.10.1)\n",
      "Requirement already satisfied: pillow>=7.1.2 in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (10.4.0)\n",
      "Requirement already satisfied: future in ./miniconda3/lib/python3.8/site-packages (from batchgenerators->acvl-utils==0.2) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (0.4)\n",
      "Requirement already satisfied: packaging>=21 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (21.0)\n",
      "Requirement already satisfied: networkx>=2.8 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (3.1)\n",
      "Requirement already satisfied: imageio>=2.27 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (2.35.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./miniconda3/lib/python3.8/site-packages (from scikit-image->acvl-utils==0.2) (2023.7.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=21->scikit-image->acvl-utils==0.2) (2.4.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn->batchgenerators->acvl-utils==0.2) (1.4.2)\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (1.13.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (3.0.0)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (2024.12.0)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (3.0.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.8/site-packages (from torch->acvl-utils==0.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->acvl-utils==0.2) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.8/site-packages (from jinja2->torch->acvl-utils==0.2) (2.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.8/site-packages (from sympy->torch->acvl-utils==0.2) (1.3.0)\n",
      "Collecting argparse\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: traceback2 in ./miniconda3/lib/python3.8/site-packages (from unittest2->batchgenerators->acvl-utils==0.2) (1.4.0)\n",
      "Requirement already satisfied: six>=1.4 in ./miniconda3/lib/python3.8/site-packages (from unittest2->batchgenerators->acvl-utils==0.2) (1.16.0)\n",
      "Requirement already satisfied: linecache2 in ./miniconda3/lib/python3.8/site-packages (from traceback2->unittest2->batchgenerators->acvl-utils==0.2) (1.0.0)\n",
      "Building wheels for collected packages: acvl-utils\n",
      "  Building wheel for acvl-utils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for acvl-utils: filename=acvl_utils-0.2-py3-none-any.whl size=22468 sha256=2530c14cef72f46d1f45bb941f0a05b4544ae31230e608a26aa534b5ce1eb31a\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/60/75/83067c05dd96f4860a844060174dcb7880f53ebfdfd559061d\n",
      "Successfully built acvl-utils\n",
      "Installing collected packages: argparse, connected-components-3d, acvl-utils\n",
      "Successfully installed acvl-utils-0.2 argparse-1.4.0 connected-components-3d-3.23.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install acvl-utils==0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04171685-5b9e-4af2-a24e-d821d484ddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NibabelIOWithReoient\n",
      "num_foreground: 11111111\n",
      "len dataset: 9\n",
      "len num_foreground_voxels: 100000000\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n",
      "Task failed with error: A process in the process pool was terminated abruptly while the future was running or pending.\n"
     ]
    }
   ],
   "source": [
    "# 前景体素数量\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from typing import Tuple, Union, List\n",
    "from typing import Type\n",
    "from nibabel.orientations import io_orientation, axcodes2ornt, ornt_transform\n",
    "import multiprocessing\n",
    "import nibabel\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import List\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from acvl_utils.cropping_and_padding.bounding_boxes import get_bbox_from_mask, bounding_box_to_slice\n",
    "import numpy as np\n",
    "num_process = 8\n",
    "preprocessed_output_folder = 'autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_preprocessed'\n",
    "num_foreground_voxels_for_intensitystats = 10e7\n",
    "properties_file = join(preprocessed_output_folder, 'dataset_fingerprint.json')\n",
    "\n",
    "overwrite_existing = False\n",
    "\n",
    "class NibabelIOWithReoient():\n",
    "    def read_images(self, image_fnames: Union[List[str], Tuple[str, ...]]) -> Tuple[np.ndarray,dict]:\n",
    "        images = []\n",
    "        original_affines = []\n",
    "        reoriented_affines = []\n",
    "        spacings_for_nnunet = []\n",
    "        for f in image_fnames:\n",
    "#           加载医学图像\n",
    "            nib_image = nibabel.load(f)\n",
    "            assert nib_image.ndim == 3, 'only 3d images'\n",
    "#           记录原始的仿射矩阵，为了之后数据处理记录原始的内容，包含体素在三维空间的坐标，方向，间距（每个体素的大小），偏移量\n",
    "            original_affine = nib_image.affine\n",
    "#           重定向，从原始仿射变换的方向转换成标准方向。\n",
    "            reoriented_image = nib_image.as_reoriented(io_orientation(original_affine))\n",
    "            reoriented_affine = reoriented_image.affine\n",
    "            \n",
    "            original_affines.append(original_affine)\n",
    "            reoriented_affines.append(reoriented_affine)\n",
    "            \n",
    "#           获取体素间距。header包含图像元数据，get_zooms()获得每个轴的体素间距，[::-1]将nibabel的（x,y,z）改为simpleITK的轴顺序（z,y,x）  \n",
    "            spacings_for_nnunet.append(\n",
    "                [float(i) for i in reoriented_image.header.get_zooms()[::-1]]\n",
    "            )\n",
    "#           调整图像数据的轴顺序get_fdata表示获取图像的numpy格式的数据，transpose是将轴的顺序转换成（z,y,x），[None]表示在开始添加一个新的维度[1,z,y,x]\n",
    "            images.append(reoriented_image.get_fdata().transpose((2,1,0))[None])\n",
    "\n",
    "            dict = {\n",
    "                'nibabel_stuff':{\n",
    "                    'original_affine':original_affines[0],\n",
    "                    'reoriented_affine':reoriented_affines[0],\n",
    "                },\n",
    "                'spacing':spacings_for_nnunet[0]\n",
    "            }\n",
    "        return np.vstack(images, dtype=np.float32, casting='unsafe'), dict\n",
    "    \n",
    "#     return a array\n",
    "    def read_seg(self, seg_fname: str) -> Tuple[np.ndarray, dict]:\n",
    "        return self.read_images((seg_fname,))\n",
    "    \n",
    "#     将分割结果保存为一个nifTI文件\n",
    "    def wirte_seg(self, seg: np.ndarray, output_fname: str, properties: dict) -> None:\n",
    "#       一个numpy数组\n",
    "        seg = seg.transpose((2,1,0)).astype(np.uint8 if np.max(seg) else np.uint16, copy=False)\n",
    "#       创建NIFTI图像对象，seg调整轴顺序的分割结果，affine是使用图像重新定向后的仿射矩阵。变成一样的，seg和img是一样的仿射变换\n",
    "        seg_nib = nibabel.NiftilImage(seg, affine=properties['nibabel_stuff']['reoriented_affine'])\n",
    "        \n",
    "#       计算原始图像的仿射矩阵\n",
    "        img_ornt = io_orientation(properties['nibabel_stuff']['original_affine'])\n",
    "    \n",
    "#         将方向代码（RAS）转换为方向矩阵\n",
    "        ras_ornt = axcodes2ornt(\"RAS\")\n",
    "# 计算从标准方向（RAS）到原始方向的转换矩阵。ras_ornt是标准方向矩阵，img_ornt是原始方向矩阵\n",
    "        from_canonical = ornt_transform(ras_ornt, img_ornt)\n",
    "#        将分割结果重新定向到原始方向\n",
    "        seg_nib_reoriented = seg_nib.as_reoriented(form_canonical)\n",
    "    \n",
    "#       验证恢复后的方向是否正确\n",
    "        if not np.allclose(properties['nibabel_stuff']['original_affine'],seg_nib_reoriented.affine):\n",
    "            print(f'warning: restored affine does not match original affine. File:{output_fname}')\n",
    "            print(f'original affine\\n', properyies['nibabel_stuff']['original_affine'])\n",
    "            print(f'restored affine\\n', seg_nib_reoriented.affine)\n",
    "#       将重新定向的分割结果保存为NIFTI文件，文件路径为output_fname\n",
    "        nibabel.save(seg_nib_reoriented, out_fname)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def determine_reader_writer_from_dataset_json(dataset_json_content: dict, example_file: str=None,\n",
    "                                              allow_normatching_filename: bool=False, verbose:bool=True):\n",
    "#     获取overwrite_image_reader_writer的键值\n",
    "    if 'overwrite_image_reader_writer' in dataset_json_content.keys() and \\\n",
    "            dataset_json_content['overwrite_image_reader_writer'] != 'None':\n",
    "        ioclass_name = dataset_json_content['overwrite_image_reader_writer']\n",
    "        print(ioclass_name)\n",
    "#     return ioclass_name\n",
    "#         try:\n",
    "#             ret 是用来确定含有NibabelIOWithReoient这个函数的\n",
    "\n",
    "#       这里返回一个读写器类，如果rw中的read_images成功执行，就会返回这个rw类别\n",
    "    if ioclass_name == 'NibabelIOWithReoient':\n",
    "#         Ni = NibabelIOWithReoient()\n",
    "#         no gpu is not work\n",
    "#         a = Ni.read_images((example_file,))\n",
    "#         print(a)\n",
    "        return NibabelIOWithReoient #返回类本身，而不是类的实例    \n",
    "    \n",
    "def analyze_case(image_files: List[str], segmentation_file: str, reader_writer_class: Type,\n",
    "                num_samples: int = 10000):\n",
    "    rw = reader_writer_class()\n",
    "    images, properties_images = rw.read_images(image_files)\n",
    "    print(images.shape)\n",
    "    print(\"hello\")\n",
    "    segmentation, properties_seg = rw.read_seg(segmentation_file)\n",
    "\n",
    "#裁剪图像和分割标签，创建一个非零掩码\n",
    "#总结：下面这段代码是使用非零掩码，去裁剪图像和标签，裁剪的图像和标签大小是一致的，相当于去除掉冗余部分，只保留组织\n",
    "    assert images.ndim in (3, 4), \"data must have shape (C,X,Y,Z) or shape (C,X,Y)\"\n",
    "    #data=(C,X,Y,Z), data[0]=(X,Y,Z),判断所有数据都不为0，结果是一个布尔数组，形状与data[0]一样,不为0的为True，为0的为False\n",
    "    non_zero_mask = images[0] != 0\n",
    "    print(non_zero_mask)\n",
    "    #data.shape[0] is C,这里就是将每个通道中的True都放在一个矩阵里面，只要有true，这个位置就是true\n",
    "    for c in range(1, images.shape[0]):\n",
    "        #将通道c的非0区域合并到nonzero_mask中\n",
    "        non_zero_mask |= images[c] !=0\n",
    "        \n",
    "    #填充二值图像中的孔洞，孔洞☞True里面的False区域，True包围的里面有false就会填充为true\n",
    "    non_zero_mask = binary_fill_holes(non_zero_mask)\n",
    "    \n",
    "    #从非零掩码中获取包含所有非零区域的的最小边界框 ，结果是bbox = (1,4,1,4,1,4)x轴从1到4，y轴从1到4,z轴从1到4\n",
    "    bbox = get_bbox_from_mask(non_zero_mask)\n",
    "    \n",
    "    #将边界框转换为切片器，用于裁剪数据，结果是（slice(1,4), slice(1,4)）\n",
    "    slicer = bounding_box_to_slice(bbox)\n",
    "    #使用切片器裁剪非零掩码，并添加一个额外的维度，以匹配通道维度,切片后的形状为(1,3,3),这里面还是掩码，就是True或false\n",
    "    nonzero_mask = nonzero_mask[slicer][None]\n",
    "    \n",
    "    #sclice data 使用切片数据裁剪输入数据data，slice(None)用于保留所有通道，裁剪后的data的形状为（c，3，3）\n",
    "    slicer = (slice(None), ) + slicer\n",
    "    images = images[slicer]\n",
    "    \n",
    "    #使用切片数据裁剪分割数据，将分割标签中为0 且不在非零掩码内的区域设置为nonzero_label\n",
    "    if segmentation is not None:\n",
    "        segmentation = segmentation[slicer]\n",
    "        #裁剪分割的=0的部分并且掩码部分为false，就会打上nonzero_label标签\n",
    "        segmentation[(segmentation == 0) & (~nonzero_mask)] = nonzero_label\n",
    "    else:\n",
    "        segmentation = np.where(nonzero_mask, np.int8(0), np.int8(nonzero_label))\n",
    "        \n",
    "#从图像中收集前景区域的强度值，并计算每个通道的强度统计信息\n",
    "    \n",
    "       \n",
    "# print(dataset[dataset.keys().__iter__().__next__()]['images'][0])\n",
    "# autodl-tmp/kits19-cnn-master/kits23-main/nnUNet_raw/imagesTr/case_00000_0000.nii.gz\n",
    "if not isfile(properties_file) or overwrite_existing:\n",
    "    reader_writer_class = determine_reader_writer_from_dataset_json(dataset_json,dataset[dataset.keys().__iter__().__next__()]['images'][0])\n",
    "#     print(reader_writer_class[0])\n",
    "# 用于强度统计，也就是判断每个案例的像素数量，强度统计是计算像素的像素值，这里是数量，不是像素值\n",
    "    num_forground_samples_per_case = int(num_foreground_voxels_for_intensitystats // len(dataset))\n",
    "    print(\"num_foreground:\",num_forground_samples_per_case)\n",
    "    print('len dataset:',len(dataset))\n",
    "    print('len num_foreground_voxels:',int(num_foreground_voxels_for_intensitystats))\n",
    "#     print(dataset[dataset.keys().__iter__().__next__()]['images'][0])\n",
    "    futures = []\n",
    "    with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "        for k in dataset.keys():\n",
    "#             print(k)\n",
    "            futures.append(executor.submit(analyze_case, dataset[k]['images'],\n",
    "                                    dataset[k]['label'], reader_writer_class, num_forground_samples_per_case))\n",
    "#             print(r)\n",
    "# 获取每个任务的结果\n",
    "        for future in futures:\n",
    "            try:\n",
    "                result = future.result()  # 等待任务完成并获取结果\n",
    "                print(result)  # 打印结果\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed with error: {e}\")\n",
    "#     with multiprocessing.get_context(\"spawn\").Pool(8) as p:\n",
    "#         for k in dataset.keys():\n",
    "# #             print(k)#case_00000,case_00001\n",
    "#             print(dataset[k]['images'])\n",
    "# #     异步任务提交到进程池（Pool），用于异步执行DatasetFingerPrintExtractor.analyze_case\n",
    "# # p用于管理进程池，starmap_async异步执行函数，返回一个asyncresult对象，表示异步任务的结果。\n",
    "#             r.append(p.starmap_async(analyze_case,((dataset[k]['images'],dataset[k]['label'],reader_writer_class,\n",
    "#                                                    num_forground_samples_per_case),)))\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60336605-6bcb-4976-ac6b-4768df0bfecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
